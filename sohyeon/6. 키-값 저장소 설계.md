# 6장 키-값 저장소 설계
- 키-값 데이터베이스라고도 불리는 비 관계형 데이터베이스이다.
- 저장소에 저장되는 값은 고유 식별자를 키로 가진다.
- 키와 값 사이의 연결 관계를 `키-값` 쌍이라고 지정한다.
- 키-값 쌍에서의 키는 유일해야 하며 해당 키에 매달린 값은 키를 통해서만 접근할 수 있다.
- 성능상의 이유로 키는 짧을수록 좋다.
- `키`는 일반 텍스트일 수도, 해시 값일 수도 있다.
- `값`은 문자열일 수도, 리스트일 수도, 객체일 수도 있다.
- Ex) 아마존 다이나모, memcached, 레디스 등

<br/>

🔽 키 값 저장소에 보관된 데이터의 사례

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/9593f931-3e93-423b-af3b-3d0c1af60dc0">

- put(key, value): 키-값 쌍을 저장소에 저장한다.
- get(key): 인자로 주어진 키에 매달린 값을 꺼낸다.

<br/>

## 문제 이해 및 설계 범위 확정

키-값 저장소의 특성
- 키-값 쌍의 크기는 10KB 이하이다.
- 큰 데이터를 저장할 수 있어야 한다.
- 높은 가용성을 제공해야 한다. 시스템에 장애가 생겨도 빨리 응답해야 한다.
- 높은 규모 확장성을 제공해야 한다. 트래픽 양에 따라 자동적으로 서버 증설/삭제가 이뤄져야 한다.
- 데이터 일관성 수준은 조정이 가능해야 한다.
- 응답 지연시간이 짧아야 한다.

<br/>

## 단일 서버 키-값 저장소
- 키-값 쌍 전부를 메모리에 해시 테이블로 저장하는 방법은 가장 직관적이다.
  - 👍 빠른 속도 보장
  - 👎 모든 데이터를 메모리 안에 두는 것이 불가능할 수 있음
- 위 문제를 해결하기 위한 개선책
  - 데이터 압축
  - 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장
- 결국은 한 대 서버로 부족한 때가 있다. 많은 데이터를 저장하기 위해 분산 키-값 저장소가 필요하다.

<br/>

## 분산 키-값 저장소
- **분산 해시 테이블**이라고도 한다.
- 키-값 쌍을 여러 서버에 분산시킨다.

<br/>

### CAP 정리 (Consistency Availability, Partition Tolerance theorem)
- 분산 시스템을 설계할 때 잘 이해해야 한다.
- 데이터 일관성, 가용성, 파티션 감내라는 3가지 요구사항을 동시에 만족하는 분산 시스템을 설계할 수는 없다.
- 각 요구사항의 의미는 다음과 같다.
  - `데이터 일관성`: 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 보게 되어야 한다.
  - `가용성`: 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받을 수 있어야 한다.
  - `파티션 감내`: 파티션은 두 노드 사이에 통신 장애가 발생했음을 의미한다. 네트워크에 파티션이 생기더라도 시스템은 계속 동작해야 한다.
 
<br/>

🔽 CAP 정리

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/36283d39-4ea3-4c6e-ad24-e9920875daa9">

- 어떤 2가지를 충족하려면 나머지 하나는 반드시 희생되어야 한다.
- `CP 시스템`: 일관성과 파티션 감내를 지원하는 키-값 저장소. 가용성을 희생한다.
- `AP 시스템`: 가용성과 파티션 감내를 지원하는 키-값 저장소. 데이터 일관성을 희생한다.
- `CA 시스템`: 일관성과 가용성을 지원하는 키-값 저장소. 파티션 감내는 지원하지 않는다.
- 통상 네트워크 장애는 피할 수 없으므로, 분산 시스템은 파티션 문제를 반드시 감내할 수 있어야 한다. (실세계에 CA 시스템 존재X)

<br/>

> ***이상적 상태***

분산 시스템에서 데이터는 보통 여러 노드에 복제되어 보관된다.

<br/>

🔽 3대의 복제(replica) 노드 n1, n2, n3에 데이터를 보관하는 상황

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/c7032637-735a-45de-a15a-98fe05613c2c">

- 이상적 환경이라면 네트워크가 파티션되는 상황은 절대로 일어나지 않는다.
- n1에 기록된 데이터는 자동적으로 n2와 n3에 복제된다. 데이터 일관성과 가용성도 만족한다.

<br/>

> ***실세계의 분산 시스템***

분산 시스템은 파티션 문제를 피할 수 없다. (문제가 발생하면 일관성과 가용성 중 하나 선택이 필요)

<br/>

🔽 n3에 장애가 발생하여 n1 및 n2와 통신할 수 없는 상황

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/bc35a7b2-2853-4c67-b6c2-54e2247ec35d">

- 클라이언트가 n1 또는 n2에 기록한 데이터는 n3에 전달되지 않는다.
- n3에 기록되었으니 아직 n1 및 n2로 전달되지 않은 데이터가 있다면 n1과 n2는 오래된 사본을 갖고 있을 것이다.
- `CP 시스템 (일관성 선택)`: 데이터 불일치를 피하기 위해 **n1과 n2에 대해 쓰기 연산을 중단**시킨다. 가용성이 깨진다.
  - 온라인 뱅킹 시스템이 계좌 최신 정보를 출력하지 않으면 안된다. 네트워크 파티션 문제가 발생하면 상황이 해결될 때까지 오류를 반환한다.
- `AP 시스템 (가용성 선택)`: 낡은 데이터를 반환할 위험이 있더라도 **계속 읽기 연산을 허용**한다. 문제 해결 후 새 데이터를 n3에 전송한다.
- 분산 키-값 저장소를 만들 때는 요구사항에 맞도록 CAP 정리를 적용해야 한다.

<br/>

### 시스템 컴포넌트
키-값 저장소 구현에 사용될 핵심 컴포넌트 및 기술
- 데이터 파티션
- 데이터 다중화
- 일관성
- 일관성 불일치 해소
- 장애 처리
- 시스템 아키텍처 다이어그램
- 쓰기 경로
- 읽기 경로

<br/>

> ***데이터 파티션***

- 대규모 애플리케이션의 경우 단순한 해결책은 데이터를 **작은 파티션들로 분할**한 다음 **여러 대 서버에 저장**하는 것이다.
- 파티션 단위로 나눌 때 2가지 문제를 중요하게 따져봐야 한다.
  - 데이터를 여러 서버에 고르게 분산할 수 있는가
  - 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가
 
<br/>

- 안정 해시를 활용해 해당 문제를 풀 수 있다.
  - 서버를 해시 링에 배치한다. (아래 예시는 8개 서버 배치)
  - 어떤 키-값 쌍을 어떤 서버에 저장할지 결정하기 위해 우선 해당 키를 같은 링 위에 배치한다. 그 지점으로부터 링을 시계 방향으로 순회하다 만나는 첫번째 서버가 해당 키-값 쌍을 저장할 서버다. 따라서 key0은 s1에 저장된다.
 
<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/908d4bd0-bbc5-4b21-b781-7557f9182a49">

<br/><br/>

- 안정 해시를 사용하여 데이터를 파티션했을 때 좋은 점
  - `규모 확장 자동화`: 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있다.
  - `다양성`: 각 서버의 용량에 맞게 가상 노드의 수를 조정할 수 있다. (고성능 서버는 더 많은 가상 노드를 갖도록 설정 가능)
 
<br/>

> ***데이터 다중화***

높은 가용성과 안정성을 확보하기 위해 데이터를 N개 서버에 비동기적으로 다중화할 필요가 있다. (`N`: 튜닝 가능한 값)

<br/>

🔽 N개 서버를 선정하는 방법

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/9d34476f-e5a7-46c3-bee5-55128bf7e1a2">

- 어떤 키를 해시 링 위에 배치한 후, 그 지점으로부터 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관한다.
- `N=3일 경우`: key0은 s1, s2, s3에 저장된다.

<br/>

가상 노드를 사용한다면 선택한 N개의 노드가 대응될 실제 물리 서버의 개수가 N보다 작아질 수 있다.
- 노드를 선택할 때 같은 물리 서버를 중복 선택하지 않도록 한다.
- 안정성을 담보하기 위해 데이터의 사본은 다른 센터의 서버에 보관하고, 센터들은 고속 네트워크로 연결한다.

<br/>

> ***데이터 일관성***

- 여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 한다.
- 정족수 합의(Quorum Consensus) 프로토콜을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다.
  - `N`: 사본 개수
  - `W`: 쓰기 연산에 대한 정족수 (쓰기 연산이 성공한 것으로 간주될 때 필요한 최소 성공 응답 개수)
  - `R`: 읽기 연산에 대한 정족수 (읽기 연산이 성공한 것으로 간주될 때 필요한 최소 서버 응답 개수)

<br/>

🔽 N=3인 경우

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/46ae62c6-a90d-47d7-a4fb-cb3d03dcf741">

- `W=1`: 쓰기 연산이 성공했다고 판단하기 위해 중재자는 최소 한 대 서버로부터 쓰기 성공 응답을 받아야 한다.
  - s1으로부터 성공 응답을 받았다면, s0과 s2로부터의 응답은 기다릴 필요 없다.
- W, R, N의 값을 정하는 것은 응답 지연과 데이터 일관성 사이의 타협점을 찾는 과정이다.
  - **W과 R의 값이 작다면**,응답 속도는 빠르다. (강한 데이터 일관성이 보장되지 않음)
  - **W과 R의 값이 크다면**, 데이터 일관성의 수준이 향상된다. (응답 속도는 느림)
 
<br/>

N, W, R 값은 어떻게 정할 수 있을까?
- `R=1, W=N`: 빠른 읽기 연산에 최적화된 시스템
- `W=1, R=N`: 빠른 쓰기 연산에 최적화된 시스템
- `W+R > N`: 강한 일관성이 보장됨 (보통 N=3, W=R=2)
- `W+R < N`: 강한 일관성이 보장되지 않음

<br/>

> ***일관성 모델: 데이터 일관성의 수준 결정***

- `강한 일관성`: 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다.
- `약한 일관성`: 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다.
- `결과적 일관성`: 약한 일관성의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영(동기화)되는 모델이다.

<br/>

> ***비 일관성 해소 기법: 데이터 버저닝***

- 데이터를 다중화하면 가용성은 높아지지만 사본 간 일관성이 깨질 가능성도 높아진다.
- 위 문제를 해소하기 위해 등장한 기술이다.
- `버저닝`: 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것
- 각 버전의 데이터는 변경 불가능하다.

<br/>

🔽 데이터의 사본이 n1과 n2에 보관된 예제

<img width="450" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/0b22b71d-f2e2-4d87-b73c-9a290ace0fbc">

- 데이터를 가져오려는 서버1과 서버2는 get("name") 연산의 결과로 **같은 값**을 얻는다.

<br/>

🔽 데이터의 일관성이 깨지는 예제

<img width="450" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/d6f6cd23-32a9-4756-b3eb-91fe59f5b6bf">

- 서버1은 "name"에 매달린 값을 "johnSanFrancisco"로 바꾸고, 서버2는 "johnNewYork"으로 변경했다. (두 연산은 동시에 발생)
- 충돌(conflict)하는 두 값을 갖게 되었다. 각각의 버전은 v1, v2라고 하자.

<br/>

**백터 시계(vector clock)** 라는 버저닝 시스템을 통해 충돌을 해결한다.
- `[서버, 버전]`의 순서쌍을 데이터에 매단 것이다.
- 어떤 버전이 선행 버전인지, 후행 버전인지, 아니면 다른 버전과 충돌이 있는지 판별하는 데 쓰인다.

<br/>

🔽 백터 시계의 수행 로직

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/ad782003-0077-415f-87b2-1deb847d48f4">

1. 클라이언트가 데이터 D1을 시스템에 기록한다. 이 쓰기 연산을 처리한 서버는 Sx이다. 따라서 백터 시계는 `D1[Sx, 1])`으로 변한다.
2. 다른 클라이언트가 데이터 D1을 읽고 D2로 업데이트한 후 기록한다. D2는 D1에 대한 변경이므로 D1을 덮어쓴다. 이때 쓰기 연산은 같은 서버 Sx가 처리한다고 가정한다. 백터 시계는 `D2([Sx, 2])`로 바뀐다.
3. 다른 클라이언트가 D2를 읽어 D3로 갱신한 다음 기록한다. 이 쓰기 연산은 Sy가 처리한다고 가정하자. 백터 시계 상태는 `D3([Sx, 2], [Sy, 1])`로 바뀐다.
4. 또 다른 클라이언트가 D2를 읽고 D4로 갱신한 다음 기록한다. 이때 쓰기 연산은 Sz가 처리한다고 가정하자. 백터 시계는 `D4([Sx, 2], [Sz, 1])`가 된다.
5. 어떤 클라이언트가 D3과 D4를 읽으면 데이터 간 충돌이 있다는 것을 알게 된다. 이 충돌은 클라이언트가 해소한 후에 서버에 기록한다. 이 쓰기 연산을 처리한 서버는 Sx였다고 하자. 백터 시계는 `D5([Sx, 3], [Sy, 1], [Sz, 1])`로 바뀐다.

<br/>

👎 백터 시계의 단점
- 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로, 클라이언트 구현이 복잡해진다.
- `[서버: 버전]`의 순서쌍 개수가 굉장히 빨리 늘어난다. (임계치 이상으로 길이가 길어지면 오래된 순서쌍을 벡터 시계에서 제거하도록 한다. 이렇게 되면 버전 간 선후 관계가 정확하지 않아 충돌 해소 과정의 효율성이 낮아진다. 하지만 실제 서비스에서 관련 문제가 발생하는 것은 발견되지 않았다고 한다.)

<br/>

> ***장애 처리***

- 장애는 흔하게 발생하기 때문에, 처리 방법은 굉장히 중요한 문제다.
- `장애 감지 기법`과 `장애 해소 전략`들을 짚어본다.

<br/>

💡 **장애 감지**
- 보통 2대 이상의 서버가 똑같이 서버 A의 장애를 보고해야 해당 서버에 실제로 장애가 발생했다고 간주한다.

<br/>

🔽 모든 노드 사이에 멀티캐스팅 채널 구축

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/a1116e94-7900-4412-a0ea-3571b8b3aab8">

- 가장 손쉬운 방법이다.
- 👎 서버가 많을 때는 비효율적이다.

<br/>

🔽 가십 프로토콜: 분산형 장애 감지 솔루션, 효율적
- 각 노드는 멤버십 목록을 유지한다. 멤버십 목록은 각 멤버 ID와 그 박동 카운터 쌍의 목록이다.
- 각 노드는 주기적으로 자신의 박동 카운터를 증가시킨다.
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보낸다.
- 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신한다.
- 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애(offline) 상태인 것으로 간주한다.

<br/>

🔽 가십 프로토콜 예제

<img width="450" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/569e7b6c-d3df-490f-aa3a-0fd12c86b405">

- 노드 s0은 그림 좌측의 테이블과 같은 멤버십 목록을 가진 상태다.
- 노드 s0은 노드 s2(멤버 ID=2)의 박동 카운터가 오랫동안 증가되지 않았다는 것을 발견한다.
- 노드 s0은 노드 s2를 포함하는 박동 카운터 목록을 무작위로 선택된 다른 노드에게 전달한다.
- 노드 s2의 박동 카운터가 오랫동안 증가되지 않았음을 발견한 모든 노드는 해당 노드를 장애 노드로 표시한다.

<br/>

💡 **일시적 장애 처리**
- 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다.
- 그동안 발생한 변경사항은 해당 서버가 복구되었을 때 일괄 반영하여 데이터 일관성을 보존한다.
- 이를 위해 임시로 쓰기 연산을 처리한 서버에는 그에 관한 단서(hint)를 남겨둔다. (**단서 후 임시 위탁**)

<br/>

🔽 단서 후 임시 위탁 예제

<img width="350" alt="image" src="https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/55437339/8c657f3f-c963-4f7e-9093-78d15a4e9397">

- 장애 상태인 노드 s2에 대한 읽기 및 쓰기 연산은 일시적으로 노드 s3가 처리한다.
- s2가 복구되면, s3는 갱신된 데이터를 s2로 인계한다.

<br/>

💡 **영구 장애 처리**

