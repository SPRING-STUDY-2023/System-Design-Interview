수평적 규모 확장성을 달성하기 위해서는 요청 또는 데이터를 서버에 균등하게 나누는 것이 중요합니다. **안정 해시**는 **이 목표를 달성하기 위해 보편적으로 사용하는 기술입니다**. 하지만 우선 이 해시 기술이 풀려고 하는 문제부터 좀 더 자세히 살펴볼까요 ?

## 해시 키 재배치(rehash) 문제

N개의 캐시 서버가 있다고 해봅시다. 이 서버들에 부하를 균등하게 나누는 보편적 방법은 아래의 해시 함수를 사용하는 것입니다.

`serverIndex = hash(key) % N`

예제를 통해 어떻게 동작하는지 알아봅시다. 총 4대의 서버를 사용한다고 했을 때 다음 표는 주어진 각각의 키에 대해서 해시 값과 서버 인덱스를 계산한 예제입니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/0e22fca3-01ab-42a4-be70-eb076a124eba)


특정한 키가 보관된 서버를 알아내기 위해, 나머지 연산을 `f(key) % 4` 와 같이 적용했습니다. 아래 그림은 위의 예제에서 키 값이 서버에 어떻게 분산되는지를 보여줍니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/10ac2d1e-60bf-4090-be55-1783b15508c8)


이 방법은 서버 풀(server pool)의 크기가 고정되어 있을 때, 그리고 데이터 분포가 균등할 때는 잘 동작합니다. 하지만 서버가 추가되거나 기존 서버가 삭제되면 문제가 생깁니다. 

예를 들어 1번 서버가 장애를 일으켜 동작을 중단했다고 해볼까요 ? 그러면 서버 풀의 크기는 3으로 변하게 되고 그 결과로, 키에 대한 해시 값은 변하지 않지만 나머지 연산을 적용하여 계산한 서버 인덱스 값은 달라집니다.

그럼 결국 장애가 발생한 1번 서버에 보관되어 있는 키 뿐만 아니라 대부분의 키가 재분배됩니다. 즉, 1번 서버가 죽으면 대부분 캐시 클라이언트가 데이터가 없는 엉뚱한 서버에 접속하게 된다는 뜻입니다. 그 결과로 대규모 캐시 미스가 발생하게 될 것입니다.

안정 해시는 바로 이 문제를 효과적으로 해결 하는 기술입니다 !

## 안정 해시

안정 해시의 정의는 다음과 같습니다.

<aside>
💡 **해시 테이블 크기가 조정될 때 평균적으로 k/n개의 키만 재배치하는 해시 기술**
*(k: 키의 개수, n은 슬롯의 개수)*

</aside>

### 해시 공간과 해시 링

이제 안정 해시의 정의는 이해했으니 그 동작 원리를 살펴볼까요 ? 해시 함수 f로는 SHA-1을 사용한다고 하고, 그 함수의 출력 값 범위는 x0, x1, x2 … xn과 같다고 해봅시다.

SHA-1의 해시 공간 범위는 $0$ 부터 $2^{160}-1$ 까지라고 알려져 있습니다. 따라서 아래 그림으로 표현할 수 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/b73fe328-ed46-4058-9ae8-38009d34d6d9)


이 해시 공간의 양쪽을 구부려 접으면 아래 그림과 같이 해시 링을 만들 수 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/740acbfa-a7fa-4a99-a54e-193d3e00c7b0)


### 해시 서버

이 해시 함수 f를 사용하면 서버 IP나 이름을 이 링 위의 어떤 위치에 대응시킬 수 있습니다. 아래 그림은 4개의 서버를 이 해시 링 위에 배치한 결과 입니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/32c26876-f36f-49e2-8734-295ccf37a0b7)


### 해시 키

여기 사용된 해시 함수는 앞서 언급한 “해시 키 재배치 문제”에서 사용한 함수와는 다르며, 나머지 연산을 사용하지 않고 있는 것입니다. 아래 그림과 같이 캐시할 키 key0, key1, key2, key3 또한 해시 링 위의 어느 지점에 배치할 수 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/fcd32a2e-01b6-45f5-ab16-e385d0c9c783)


### 서버 조회

어떤 키가 저장되는 서버는 해당 키의 위치로부터 시계 방향으로 링을 탐색해 나가면서 만나는 첫번째 서버입니다. 아래 그림이 그 과정을 보여주고 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/bbb590ea-94bf-4049-afa8-e2a16e474a7a)


### 서버 추가

방금 설명한 내용에 따르면, 서버를 추가하더라도 키 가운데 일부만 재배치 하면 됩니다. 그림에서는 새로운 서버 4가 추가된 뒤에 key0만 재배치 됨을 알 수 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/87f01141-1a97-45d0-941f-d09b55da70fe)


### 서버 제거

하나의 서버가 제거되면 키 가운데 일부만 재배치 됩니다. 아래 그림을 보면 서버 1이 삭제되었을 때 key1만이 서버 2로 재배치 됩니다. 나머지 키는 영향이 없는 것이죠.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/2a183982-dd76-4069-86ee-4c01f0e6df37)


### 기본 구현법의 두 가지 문제

안정 해시 알고리즘은 MIT에서 처음 제안 되었습니다. 그 기본 절차는 다음과 같습니다.

- 서버와 키를 균등 분포 해시 함수를 사용해 해시 링에 배치한다.
- 키의 위치에서 링을 시계 방향으로 탐색하다 만나는 최초의 서버가 키가 저장될 서버다.

하지만 이 접근법에는 두 가지 문제가 있습니다. 

**서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기를 균등하게 유지하는 게 불가능하다는 것이 첫번째 문제**입니다. 여기서 파티션은 인접한 서버 사이의 해시 공간을 뜻합니다. 어떤 서버는 굉장히 작은 해시 공간을 할당 받고, 어떤 서버는 굉장히 큰 해시 공간을 할당 받는 상황이 가능한 것이죠. 아래 그림은 s2이 삭제되는 바람에 s2의 파티션이 다른 파티션 대비 거의 2배로 커지는 상황을 보여줍니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/49f7672a-df18-4a24-a3df-36afbac4f579)


**두번째 문제는 키의 균등 분포를 달성하기 어렵다는 것**입니다. 예를 들어 서버가 아래 그림과 같이 배치 되어 있다면, 서버 1과 서버 3은 아무 데이터도 갖지 않는 반면, 대부분의 키는 서버 2에 보관될 것입니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/f6cfc5a2-445a-46bd-9ca5-16939f230138)


이 문제를 해결하기 위해 제안된 기법이 **바로 가상 노드 또는 복제라고 불리는 기법**입니다.

### 가상 노드

가상 노드는 실제 노드 또는 서버를 가리키는 노드로서, 하나의 서버는 링 위에 여러 개의 가상 노드와 매핑될 수 있습니다. 아래 그림을 보면 서버 0과 서버 1은 3개의 가상 노드를 갖습니다. 여기서 노드 개수 3은 임의로 정한 것이며, 실제 시스템에서는 그보다 훨씬 더 큰 값이 사용됩니다.

즉, 서버 0을 링에 배치하기 위해 s0 하나만 쓰는게 아니라 s0_0, s0_1, s0_2의 세 개의 가상 노드를 사용하는 것입니다. 따라서 각 서버는 하나가 아닌 여러 개 파티션을 관리해야 합니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/302136fe-5b69-4a87-af84-787d74bf05d0)


키의 위치로부터 시계방향으로 링을 탐색하다가 만나는 최초의 가상 노드가 해당 키를 저장할 서버가 되는 것이죠 ! 그렇게 되면 아래와 같은 규칙으로 할당이 됩니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/ae339946-7cb2-4649-82ff-e67a20de7702)


가상 노드의 개수를 늘리면 키의 분포는 점점 더 균등해집니다. 표준 편차가 작아져서 데이터가 고르게 분포되기 때문이죠. 표준 편차는 데이터가 어떻게 퍼져 나갔는지를 보이는 척도입니다. 100개 ~ 200개의 가상 노드를 사용했을 경우 권장하는 표준 편차 값은 평균의 5%(가상 노드가 200개인 경우)에서 10%(가상 노드가 100개인 경우) 사이입니다.

가상 노드의 개수를 더 늘리면 표준 편차의 값은 더 떨어지게 되지만 가상 노드 데이터를 저장할 공간은 더 많이 필요하게 될 것입니다. 즉, tradeoff가 필요하다는 뜻이죠. 그렇게 때문에 시스템 요구사항에 맞도록 가상 노드 개수를 적절히 조정해야 합니다.

### 재배치할 키 결정

서버가 추가되거나 제거 되면 데이터 일부는 재배치를 해야합니다. 그럼 어느 범위의 키들이 재배치 되어야 할까요 ?

서버 4가 추가 되었다고 했을 때, 아래 그림을 보면 영향 받은 범위는 s4와 그 반시계 방향에 있는 첫번째 s3 서버까지임을 확인할 수 있습니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/23177250-bb6d-49a1-9fcf-87e6fbe2926f)


서버 삭제 된 예제도 아래 처럼 확인할 수 있습니다. 서버 s1이 삭제된다면 s1부터 그 반시계 방향에 있는 최초 서버 s0 사이에 있는 키들이 s2로 재배치 되어야 합니다.

![image](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/68415644/e9f2b8c7-92b6-468f-afd2-267d3a9f8e81)
