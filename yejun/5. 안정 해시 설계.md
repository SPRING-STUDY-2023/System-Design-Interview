# 5장. 안정 해시 설계
수평적 규모 확장성을 달성하기 위해서, 요청 or 데이터를 서버에 균등하게 나누는 것이 중요하다!

→ 이를 위해 보편적으로 사용하는 기술이 바로 ***안정 해시***

## 해시 키 재배치(rehash) 문제

$$
serverIndex = hash(key) \% N 
$$

*N = 캐시 서버의 개수

서버들에 부하를 균등하게 나누려면, 위와 같은 해시 함수를 보편적으로 사용한다. 

서버의 개수만큼 나머지 연산을 하여 특정한 키를 보관하는 방식은 **①서버 풀(server pool)의 크기가 고정되어 있을 때  ②데이터 분포가 균등할 때** 잘 동작한다. 

하지만, 서버가 추가되거나 기존 서버가 삭제되면(= 서버의 개수가 변하면) 문제가 생긴다. 

→ 나머지 연산의 결과가 달라지기 때문! (서버 인덱스 값도 달라지게 됨)

대부분의 키가 재분배 되고 나면, 캐시 클라이언트가 데이터가 없는 엉뚱한 서버에 접속하게 되는 **대큐모 캐시 미스(cache miss)**로 이어진다. 

> 안정해시는 이러한 문제를 효과적으로 해결하는 기술이다!
> 

## 안정 해시

해시 테이블의 크기가 조정될 때 평균적으로 오직 k/n개의 키만 재배치하는 해시 기술

*k = 키의 개수, n = 슬롯(slot)의 개수

### 해시 공간과 해시 링

<aside>
📎 [예시] 해시 함수 SHA-1

- 함수의 출력 값 범위 : x0, x1, x2, x3, …, xn
- 해시 공간(hash space)의 범위 : 0 ~ 2^160-1

x0 = 0 ,  xn = 2^160-1

</aside>

해시 공간을 위와 같이 구부려 접으면, 해시 링(hash ring)이 만들어진다. 

![Untitled](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/80024278/56ffb002-6dec-42d7-9456-bf3197aae8bd)

### 해시 서버

해시 함수 f를 사용하면, 서버 IP나 이름을 링 위의 특정 위치에 대응시킬 수 있다. 

**이때 위에서 나온 문제의 나머지 연산(modular) 함수는 사용하지 않는다.* 

### 해시 키

캐시할 키 key0, key1, key2, key3 등을 해시 링 위의 어느 지점에 배치할 수 있다. 

### 1. 서버 조회

> 어떤 키가 저장되는 서버는 해당 키의 위치로부터 **시계 방향**으로 링을 탐색해 나가면서 만나는 **첫 번째 서버**
> 

key0 → 서버0, key1 → 서버1, key2 → 서버2, key3 → 서버3에 저장!

### 2. 서버 추가

> 서버를 추가하더라도 키 가운데 일부만 재배치하면 된다!
> 

기존의 서버0에 저장되어 있던 key0은 추가된 서버4를 가장 만나게 되므로, key0만 서버4에 저장되도록 바뀌고 **나머지는 변하지 않는다.**

![Untitled](https://github.com/SPRING-STUDY-2023/System-Design-Interview/assets/80024278/02d5ee6d-4660-4dbb-834c-651b5d2f01c6)

### 3. 서버 제거

> 하나의 서버가 제거되면 키 가운데 일부만 재배치된다.
> 

기존의 key가 저장된 서버가 삭제된다면, 해당 key는 다시 자신의 위치를 기준으로 가까운 서버를 찾아 저장하도록 바뀐다. 마찬가지로, **나머지 키에는 영향이 없다.** 

### ⚠️ 기본 구현법의 2가지 문제

<aside>
📍 **안정 해시 알고리즘의 기본 절차**

1. 서버와 키를 균등 분포 해시 함수를 사용해 해시 링에 배치한다.
2. 키의 위치에서 링을 시계 방향으로 탐색하다 만나는 최초의 서버가 키가 저장될 서버다.
</aside>

위 접근법에는 두 가지 문제가 있다. 

1. 서버가 추가되거나 삭제되는 상황을 감안하면 파티션의 크기를 균등하게 유지하는 게 불가능하다
    
    ****파티션(partition) : 인접한 서버 사이의 해시 공간***
    
    ⇒ 어떤 서버는 매우 큰 or 작은 해시 공간을 할당 받는 상황이 가능하다는 것이다. 
    
2. 키의 균등 분포를 달성하기가 어렵다
    
    즉, 특정 서버에만 보관될 키가 몰릴 수 있는 상황이다. 
    

> 위 문제를 해결하기 위해 제안된 기법이 가상 노드(virtual node) 또는 복제(replica)이다!
> 

### 가상 노드

실제 노드 또는 서버를 가리키는 노드 → 하나의 서버는 링 위에 여러 개의 가상 노드를 가질 수 있다. 

서버마다 3개의 가상 노드를 가진다고 하면, s0_0, s0_1, s0_2와 같이 여러 개의 파티션을 관리해야 한다. 

이때는 키의 위치로부터 시계방향으로 **링을 탐색하다 만나는 최초의 가상 노드가 해당 키가 저장될 서버**가 된다. 

가상 노드의 개수를 늘리면 키의 분포는 점점 더 균등해진다. 

→ 표준 편차가 작아져서 데이터가 고르게 분포되기 때문 

 ****표준 편차: 데이터가 어떻게 퍼져 나갔는지를 보이는 척도***

ex. 100~200개의 가상 노드 - 표준 편차 값: 평균 5%~10%

> **가상 노드 개수가 늘어날수록 표준 편차의 값은 떨어진다.**
> 

### 재배치할 키 결정

서버가 추가되거나 제거되면 데이터 일부는 재배치해야 한다. 

***어떤 기준?***

추가된 노드(서버)부터 반시계 방향에 있는 첫 번쨰 서버까지!

## [정리] 안정 해시의 이점

- 서버가 추가되거나 삭제될 때 재배치되는 키의 수가 최소화된다
- 데이터가 보다 균등하게 분포하게 되므로 수평적 규모 확장성을 달성하기 쉽다
- 핫스팟 키 문제를 줄인다
    
    특정 shard에 대한 접근이 지나치게 빈번하면 서버 과부하 문제가 생길 수 있는데, 안정 해시는 데이터를 좀 더 균등하게 분배하게 되므로 이런 문제가 생길 가능성을 줄인다. 
    

### 유명한 기술 예시

- Amazon DynamoDB의 파티셔닝 관련 컴포넌트
- Apache Cassandra 클러스터에서의 데이터 파티셔닝
- Discord 채팅 어플리케이션
- Akamai CDN
- Meglev 네트워크 부하 분산기
